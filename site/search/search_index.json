{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Clustering/","text":"Clustering \u200b Clustering Clustering adalah sebuah metode untuk mengelompokkan data berdasarkan sebuah ketentuan . Misal , data tersebut kelompokkan berdasarkan wilayah , jenis berita , dan pengelompokan berdasarkan tempat berita tersebut dilaporkan. Clustering Dengan Pendekatan Partisi : K-Means Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Fuzzy K-Means Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Shilhoutte Coefficient Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i . Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S i = (b i \u2013 a i ) / max(a i , b i ) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a i < b i ) dan a i mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a i = 0. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Clustering"},{"location":"Clustering/#clustering","text":"\u200b","title":"Clustering"},{"location":"Clustering/#clustering_1","text":"Clustering adalah sebuah metode untuk mengelompokkan data berdasarkan sebuah ketentuan . Misal , data tersebut kelompokkan berdasarkan wilayah , jenis berita , dan pengelompokan berdasarkan tempat berita tersebut dilaporkan. Clustering Dengan Pendekatan Partisi :","title":"Clustering"},{"location":"Clustering/#k-means","text":"Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya Alokasikan kembali semua data ke cluster terdekat Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Mixture Modelling (Mixture Modeling) Mixture modelling (mixture modeling) merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Code dibawah ini digunakan untuk melakukan clustering, sebagai contoh, cluster dibagi menjadi 5. Namun, nantinya cluster dapat diubah sesuai dengan kebutuhan. # Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(fiturBaru) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i]))","title":"K-Means"},{"location":"Clustering/#fuzzy-k-means","text":"Metode Fuzzy K-Means (atau lebih sering disebut sebagai Fuzzy C-Means ) mengalokasikan kembali data ke dalam masing-masing cluster dengan memanfaatkan teori Fuzzy. Teori ini mengeneralisasikan metode pengalokasian yang bersifat tegas (hard) seperti yang digunakan pada metode Hard K-Means. Dalam metode Fuzzy K-Means dipergunakan variabel membership function, uik, yang merujuk pada seberapa besar kemungkinan suatu data bisa menjadi anggota ke dalam suatu cluster. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system.","title":"Fuzzy K-Means"},{"location":"Clustering/#shilhoutte-coefficient","text":"Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i . Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S i = (b i \u2013 a i ) / max(a i , b i ) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a i < b i ) dan a i mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a i = 0. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Berikut adalah nilai silhoutte berdasarkan Kaufman dan Rousseeuw : 0.7 < SC <= 1 Strong Stucture 0.5 < SC <= 0.7 Medium Structure 0.25 < SC <= 0.5 Weak Structure SC <= 0.25 No structure Code untuk shilhoutte coefficient : s_avg = silhouette_score(fiturBaru, kmeans.labels_, random_state=10)","title":"Shilhoutte Coefficient"},{"location":"Ekstraksi_Teks/","text":"Ekstraksi Teks Tahapan selanjutnya adalah Ekstraksi Teks. Yaitu memfilter kata sambung/kata-hubung yang tidak diperlukan dalam proses web mining. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklear. Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Lalu, tampilkan data tersebut ke dalam file .csv dengan code sebagai berikut : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Perhitungan TFIDF Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Ini adalah bentuk codenya : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#ekstraksi-teks","text":"Tahapan selanjutnya adalah Ekstraksi Teks. Yaitu memfilter kata sambung/kata-hubung yang tidak diperlukan dalam proses web mining. Tahapan - tahapan dalam ekstraksi teks yaitu : Tahap Stop word Removal Stop word adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop word umumnya dimanfaatkan dalam task information retrieval. Contoh stop word untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. Tahap Stemming Stemming merupakan pemotongan suatu kata menjadi kata dasar, jadi jika terdapat kata imbuhan, maka imbuhan akan dihilangkan, dan hanya diambil kata dasarnya saja. Contohnya kata \"tersaring\" menjadi \"saring\". Agar dapat melakukan kedua tahapan diatas, maka perlu menginstall library Sastrawi dan Sklearn. Caranya adalah, pertama - tama membuka command prompt di laptop anda : Ketikkan kode berikut : pip install Sastrawi pip install Sastrawi digunakan untuk menginstall Sastrawi. pip install Sklearn pip install Sklearn digunakan untuk menginstall Sklear. Setelah Sastrawi dan Sklearn telah terinstall, kalian bisa membuka Python, lalu Ketikkan code dibawah ini : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # For processing purpose from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah ketikkan code dibawah ini : #VSM def countWord(txt): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM.append([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); # memasukkan kata baru for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) VSM[-1].append(d.get(i)) cursor = conn.execute(\"SELECT * from Berita\") cursor = cursor.fetchall() #cursor = cursor[:10] pertama = True corpus = list() c=1 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) ''' d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) ''' Lalu, tampilkan data tersebut ke dalam file .csv dengan code sebagai berikut : def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"Ekstraksi Teks"},{"location":"Ekstraksi_Teks/#perhitungan-tfidf","text":"Menghitung Term Frequency (tf). Term frequency (tf) merupakan frekuensi kemunculan term (t) pada dokumen (d). Ini adalah bentuk codenya : # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a')","title":"Perhitungan TFIDF"},{"location":"Kesimpulan/","text":"Kesimpulan Dari Hasil yang sudah kita lakukan , dapat disimpulkan bahwa Web Mining dengan menggunakan python(atau bahasa pemrograman lainnya), merupakan sebuah metode yang sangat efektif untuk mengumpulan data yang berjumlah besar dalam waktu yang sangat singkat . Apalagi data yang di ambil bisa langsung di proses sehingga menjadi sebuah informasi yang utuh dan akurat . Tetapi , beberapa website dan instansi melarang 'Web Mining' ini karena , data yang sudah di dapatkan bisa dimanfaatkan untuk hal-hal yang negatif . Bahkan tindakan yang telalu jauh bisa di sebut sebagai pencurian data . Jadi saya harap tutorial ini bisa dimanfaatkan untuk hal-hal yang positif dan bisa membantu pihak manapun. Baik buruknya program bergantung niat menggukanannya.","title":"Kesimpulan"},{"location":"Kesimpulan/#kesimpulan","text":"Dari Hasil yang sudah kita lakukan , dapat disimpulkan bahwa Web Mining dengan menggunakan python(atau bahasa pemrograman lainnya), merupakan sebuah metode yang sangat efektif untuk mengumpulan data yang berjumlah besar dalam waktu yang sangat singkat . Apalagi data yang di ambil bisa langsung di proses sehingga menjadi sebuah informasi yang utuh dan akurat . Tetapi , beberapa website dan instansi melarang 'Web Mining' ini karena , data yang sudah di dapatkan bisa dimanfaatkan untuk hal-hal yang negatif . Bahkan tindakan yang telalu jauh bisa di sebut sebagai pencurian data . Jadi saya harap tutorial ini bisa dimanfaatkan untuk hal-hal yang positif dan bisa membantu pihak manapun. Baik buruknya program bergantung niat menggukanannya.","title":"Kesimpulan"},{"location":"Preprocessing/","text":"Preprocessing Preprocessing bertujuan untuk membersihkan data dari kata yang error, tidak terdefinisi , dan lain-lain . Setelah preprocessing dilakukan maka kita akan mendapatkan data yang bersih dan bebas dari noise , sehingga data tersebut bisa di gunakan untuk di cluster Code Prepocessing Berikut adalah code untuk preprocessing data def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru","title":"Preprocessing"},{"location":"Preprocessing/#preprocessing","text":"Preprocessing bertujuan untuk membersihkan data dari kata yang error, tidak terdefinisi , dan lain-lain . Setelah preprocessing dilakukan maka kita akan mendapatkan data yang bersih dan bebas dari noise , sehingga data tersebut bisa di gunakan untuk di cluster","title":"Preprocessing"},{"location":"Preprocessing/#code-prepocessing","text":"Berikut adalah code untuk preprocessing data def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru if u%50 == 0 : print(\"seleksi : \", u, data.shape) u+=1 return data,kataBaru","title":"Code Prepocessing"},{"location":"Referensi/","text":"Referensi Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://codeburst.io/web-crawling-and-scraping-in-python-7116b16d27c7 https://scrapy.org/ https://github.com/grangier/python-goose https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3 https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097 https://github.com/samyak24jain/FuzzyCMeans https://www.learndatasci.com/tutorials/k-means-clustering-algorithms-python-intro/","title":"Referensi"},{"location":"Referensi/#referensi","text":"Berikut ini terlampir referensi dari tahapan - tahapan yang telah dibuat : https://codeburst.io/web-crawling-and-scraping-in-python-7116b16d27c7 https://scrapy.org/ https://github.com/grangier/python-goose https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3 https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097 https://github.com/samyak24jain/FuzzyCMeans https://www.learndatasci.com/tutorials/k-means-clustering-algorithms-python-intro/","title":"Referensi"},{"location":"Web Crawling/","text":"Web Crawling Web Crawling adalah sebuah aktivitas mengumpulkan informasi-informasi yang akan di proses. Informasi ini adalah informasi yang masih bersifat mentah , langsung dari web . Dan informasi ini biasanya bersifat acak dan kita biasa sebut dengan 'data'. Tutorial Web Crawling Berikut adalah tutorial meng-crawl data dari Website dan data yang sudah di crawl akan tersimpan di database . Kita akan menggukan python sebagai bahasa pemrograman dan sqlite sebagai database. Dibawah ini adalah tahapan - tahapan untuk mengcrawl data : Pertama download Python(Disini saya menggunakan versi 3) Kemudian Instal python sesuai instruksi Sebelum menjalankan Python, anda perlu menginstall library request dan beautifulsoup dengan cara : Buka Command Prompt(CMD) Ketikkan kode berikut : pip install bs4 pip install bs4 digunakan untuk menginstall beautifulsoup. pip install requests pip install request digunakan untuk menginstall requests Kita menggunakan beautifulsoup untuk mengubah objek file ke beautifulsoup, sedangkan requests untuk mengambil data dari internet Ketikkan code dibawah ini : import requests from bs4 import BeautifulSoup import sqlite3 Code diatas digunakan untuk mengambil semua library yang telah terinstall Setelah mengimport seluruh library yang dibutuhkan, maka langkah selanjutnya adalah mengkoneksikan database ke file .db yang telah dibuat sebelumnya, codenya seperti dibawah ini : conn = sqlite3.connect('test.db') disini kita melakukan pengecekan pada table 'Berita', codenya seperti dibawah ini : conn.execute('drop table if exists Berita') Membuat tabel dengan nama sesuai keinginan, tabel disini saya beri nama \"Berita\" dengan field \"Judul\" dan \"Isi\". Codenya seperti dibawah ini : conn.execute('''CREATE TABLE Berita (Judul TEXT NOT NULL, Isi TEXT NOT NULL);''') Membuat variabel yang isinya adalah link URL dari website yang ingin kita ambil datanya. Codenya seperti dibawah ini : src = \"https://news.detik.com/internasional?tag_from=wp_news_firstnav_Internasional/\" Link URL yang kita gunakan kali ini adalah [ https://news.detik.com/internasional?tag_from=wp_news_firstnav_Internasional Setelah itu kita melakukan konversi pada link URL diatas dan mengubahnya ke objek beautifulsoup. Codenya seperti dibawah ini : page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') FIeld yang telah kita buat tadi yaitu \"Judul\" dan \"Isi\" akan mengambil data berdasarkan class yang terdapat pada Website yang kita ambil. Untuk mengetahui class tersebut, kita cukup memblok teks yang akan kita crawl lalu klik kanan, setelah itu klik \"Inspect\" maka akan muncul class yang ingin kita ambil. Codenya seperti dibawah ini : judul = soup.findAll(class_='post-title entry-title') isi = soup.findAll(class_='snippets') Dengan menggunakan perulangan for, kita akan membuat variabel baru yang bernama a dan b kemudian mendeklarasikan ke tipe data \"string\" dengan deklarasi \"%s\" dan \"%s\". Codenya seperti dibawah ini : for i in range(len(judul)): a = judul[i].getText() b = isi[i].getText() conn.execute('INSERT INTO Berita(Judul, Isi) VALUES (\"%s\", \"%s\")' %(a, b)); Membuat variabel yang isinya akan menampilkan seluruh isi yang terdapat pada tabel yang bernama \"Berita\". Codenya seperti dibawah ini : cursor = conn.execute(\"SELECT * from Berita\") Menampilkan baris yang telah diambil pada tabel \"Berita\". Codenya seperti dibawah ini : for row in cursor: print(row)","title":"Web Crawling"},{"location":"introduction/","text":"Ananda Dharma Prayitno 160411100134 Penambangan dan Pencarian Web - Dosen Pengampu Bapak Mula'ab, S.si, M.kom Teknik Informatika Universitas Trunojoyo Madura Pengantar Web Mining Web Mining adalah sebuah teknik untuk melakukan penambangan data dan untuk menemukan pola dari sebuah website. Seperti namanya ,ini adalah informasi yang dikumpulkan dengan menambang setiap informasi yang di butuhkan pada sebuah web . Dan disini , kita menambang informasi tersebut secara otomatis dan terprogram dengan membuat sebuah aplikasi web mining. Tools & Library Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy)","title":"Pengantar"},{"location":"introduction/#ananda-dharma-prayitno","text":"","title":"Ananda Dharma Prayitno"},{"location":"introduction/#160411100134","text":"","title":"160411100134"},{"location":"introduction/#penambangan-dan-pencarian-web-dosen-pengampu-bapak-mulaab-ssi-mkom","text":"","title":"Penambangan dan Pencarian Web - Dosen Pengampu Bapak Mula'ab, S.si, M.kom"},{"location":"introduction/#teknik-informatika-universitas-trunojoyo-madura","text":"Pengantar Web Mining Web Mining adalah sebuah teknik untuk melakukan penambangan data dan untuk menemukan pola dari sebuah website. Seperti namanya ,ini adalah informasi yang dikumpulkan dengan menambang setiap informasi yang di butuhkan pada sebuah web . Dan disini , kita menambang informasi tersebut secara otomatis dan terprogram dengan membuat sebuah aplikasi web mining.","title":"Teknik Informatika Universitas Trunojoyo Madura"},{"location":"introduction/#tools-library","text":"Menggunakan bahasa pemrograman Python versi 3.6 Library yang digunakan untuk crawling : requests BeautifulSoup4 (bs4) Library yang digunakan untuk preprocessing : Sastrawi Library yang digunakan untuk seleksi fitur & clustering : scipy numpy sklearn (Scikit-learn) skfuzzy (Scikit-fuzzy)","title":"Tools &amp; Library"}]}